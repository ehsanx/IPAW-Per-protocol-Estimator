[["index.html", "Illustrative example of Per-protocol effect estimation in R Citation", " Illustrative example of Per-protocol effect estimation in R Lucy Mosquera &amp; Ehsan Karim 2022-12-21 Citation Supplementary materials. Check the main article for the analytic and theoretical details, and complete citations from the literature. How to cite Mosquera, L and Karim, ME (2022) “Illustrative example of Per-protocol effect estimation in R”, URL: ehsanx.github.io/IPAW-Per-protocol-Estimator "],["purpose.html", "Purpose", " Purpose The purpose of this supplement is to illustrate the estimators used in the article. This code and associated example dataset, is based on the Diagram 1 (i) data generating mechanism/simulation outlined in our manuscript; with true log(OR) = 0.7, and show how to calculate intention to treat (ITT), naive per-protocol (naive PP), baseline adjusted per-protocol and stabilized inverse probability weighted (IPW) adjusted per-protocol effect estimates. These estimators can be used for analyzing pragmatic trials in the presence of non-adherence. All computations were done in R version 3.6.1. "],["data-setup.html", "Data Setup", " Data Setup The analyses conducted in this supplement will require loading the following packages: ## Load required packages suppressPackageStartupMessages(library(lmtest)) suppressPackageStartupMessages(library(tidyr)) suppressPackageStartupMessages(library(sandwich)) suppressPackageStartupMessages(library(dplyr)) suppressPackageStartupMessages(library(ggstance)) suppressPackageStartupMessages(library(ggplot2)) suppressPackageStartupMessages(library(jtools)) Next, by setting the filePath variable to match the location of the supplied Pragmatic Trial Data.csv, we can open the dataset. By printing the first few rows, we can start getting ourselves familiar with the data. ## Load the dataset simulated.data &lt;- read.csv(paste0(filePath, &quot;data/Pragmatic Trial Data.csv&quot;)) head(simulated.data) ## Z id t0 B L1 L2 L2lag cavgL1 A Alag1 Y ## 1 1 1 0 0.2780523 2.64317182 1 0 2.643172 1 1 0 ## 2 1 1 1 0.2780523 6.14841344 1 1 4.395793 1 1 0 ## 3 1 1 2 0.2780523 1.56694834 1 1 3.452845 1 1 0 ## 4 1 1 3 0.2780523 -0.03690539 1 1 2.580407 1 1 0 ## 5 1 1 4 0.2780523 0.32103495 1 1 2.128533 1 1 0 ## 6 1 1 5 0.2780523 -2.22462997 0 1 1.403006 1 1 0 We see that this dataset consists of 11 variables, including: Z the arm the patient was randomized to id the patient id that groups observations from the same individual together t0 the time point of the observation B the measured baseline covariate L1 and L2 the time-varying covariates, with L2lag and cavgL1 derived from these columns A whether the patient received the treatment for the time interval prior to this observation Y whether the individual experienced the outcome of interest "],["data-exploration.html", "Data Exploration", " Data Exploration To better understand the dataset, we can explore the distribution of patients and observations per patient. ## Trial characteristics n &lt;- length(unique(simulated.data$id)) n1 &lt;- length(unique(simulated.data$id[simulated.data$Z==1])) n0 &lt;- length(unique(simulated.data$id[simulated.data$Z==0])) averageObs &lt;- mean(table(simulated.data$id)) cat(n, &quot;participants total&quot;, &quot;\\n&quot;, n1, &quot;were randomized to the treatment arm&quot;, &quot;\\n&quot;, n0, &quot;were randomized to the control arm.&quot;, &quot;\\n&quot;, &quot;On average each patient has &quot;, round(averageObs, 1), &quot;observations&quot;) ## 2000 participants total ## 1000 were randomized to the treatment arm ## 1000 were randomized to the control arm. ## On average each patient has 58.8 observations There are 1000 individuals in each treatment arm with approximately 59 observations per person. The proportion of trial participants who experience the event of interest in the trial dataset is: ## Event rate e.rate &lt;- sum(simulated.data$Y)/ length(unique(simulated.data$id)) cat(&quot;Event rate is&quot;, round(e.rate*100,1), &quot;%\\n&quot;) ## Event rate is 11.7 % Next, we can quantify non-adherence within the trial by calculating what proportion of individuals became non-adherent by their last observation: ## Non-adherence rates by arm ## how many person-time deviated in the treatment arm? simulated.data.treated &lt;- simulated.data[simulated.data$Z==1 &amp; simulated.data$Alag1==1,] ## how many person-time deviated in the control arm? simulated.data.control &lt;- simulated.data[simulated.data$Z==0 &amp; simulated.data$Alag1==0,] pt1dev = dim(simulated.data.treated[simulated.data.treated$A==0&amp; simulated.data.treated$t0&lt;59,])[1]/n1 pt0dev = dim(simulated.data.control[simulated.data.control$A==1&amp; simulated.data.control$t0&lt;59,])[1]/n0 cat(&quot;In the treatment arm&quot;, pt1dev*100, &quot;% were non-adherent by their last observation&quot;, &quot;\\n&quot;, &quot;In the control arm&quot;, pt0dev*100, &quot;% were non-adherent by their last observation&quot;) ## In the treatment arm 30.3 % were non-adherent by their last observation ## In the control arm 27.2 % were non-adherent by their last observation As the last part of our exploration, let’s see the covariates for this dataset and how they are distributed between the two treatment arms. In this example trial, there is one measured baseline covariate and two time-varying covariates. The estimation methods detailed in this document can be expanded to handle additional baseline or time-varying covariates by using the complete set of observed baseline covariates wherever \\(B\\) is used, and the complete set of observed time-varying covariates whereever \\(L_{1}\\) and \\(L_{2}\\) are used. simulated.data %&gt;% group_by(Z) %&gt;% summarize(B_mean = mean(B), B_sd = sd(B), L1_mean = mean(L1), L1_sd = sd(L1), L2_prop = sum(L2)/n()) ## # A tibble: 2 × 6 ## Z B_mean B_sd L1_mean L1_sd L2_prop ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.498 0.223 4.02 2.62 0.708 ## 2 1 0.486 0.219 1.97 2.58 0.472 We can see the baseline covariate is well balanced between the two treatment arms while the time-varying covariates are not well balanced between the treatment arms. This can occur when the time-varying covariates are associated with the receipt of treatment. "],["intention-to-treat-estimation.html", "Intention to Treat Estimation", " Intention to Treat Estimation The first treatment effect is the Intention To Treat (ITT) effect estimate. This quantifies the causal effect of being randomized to the treatment arm vs. the control arm. In trials with perfect adherence, where all patients receive the treatment they are randomized to receive, this corresponds with the causal effect of receiving treatment. To estimate the ITT effect, we use pooled logistic regression of the form: \\[logit(Y) = \\gamma_{0} + \\gamma_{Z} Z + \\gamma_{t}t.\\] Which is estimated via the following code: ITTFit &lt;- glm(Y ~ t0 + Z, data = simulated.data, family = binomial(link=&quot;logit&quot;)) ITTFit ## ## Call: glm(formula = Y ~ t0 + Z, family = binomial(link = &quot;logit&quot;), ## data = simulated.data) ## ## Coefficients: ## (Intercept) t0 Z ## -9.97641 0.09086 0.18270 ## ## Degrees of Freedom: 117517 Total (i.e. Null); 117515 Residual ## Null Deviance: 3378 ## Residual Deviance: 3034 AIC: 3040 To calculate the appropriate standard error, p-values, and confidence intervals, extra consideration must be used to account for the clustering of observations in the dataset. As we are using pooled logistic regression, each observation corresponds to a single individual at a given follow-up. This means that observations are not all independent, as we’d expect observations from the same individual to be more alike other than observations from different individuals. We calculate the appropriate uncertainty for each estimate using clustered sandwich estimates of the standard error: cov.m1 &lt;- vcovCL(ITTFit, type = &quot;HC0&quot;, cluster= simulated.data$id) co.test &lt;- coeftest(ITTFit, vcov = cov.m1) co.CI &lt;- coefci(ITTFit, vcov = cov.m1, level = 0.95) cat(&quot;ITT effect estimate:&quot;, &quot;\\n&quot;, &quot;log(OR)&quot;, round(coefficients(ITTFit)[[&quot;Z&quot;]],3), &quot;\\n&quot;, &quot;95% CI:&quot;, round(co.CI[&quot;Z&quot;,&quot;2.5 %&quot;],3), &quot;,&quot;, round(co.CI[&quot;Z&quot;,&quot;97.5 %&quot;],3), &quot;\\n&quot;, &quot;p-value&quot;, round(co.test[&quot;Z&quot;,&quot;Pr(&gt;|z|)&quot;],2), &quot;\\n&quot;, &quot;robust standard error&quot;, round(sqrt(diag(cov.m1))[[&quot;Z&quot;]],3)) ## ITT effect estimate: ## log(OR) 0.183 ## 95% CI: -0.075 , 0.441 ## p-value 0.17 ## robust standard error 0.132 Our ITT estimate concludes that treatment does not have a statistically significant effect on the outcome. We have showed the calculation above in details to explain how exactly clustered sandwich estimates of the standard error are calculated to make proper inference. Alternatively, we can use summ() function from jtools package to perform the above calculations for log(OR) in a straightforward manner by specifying appropriate arguments. require(jtools) summ(ITTFit, robust = &quot;HC0&quot;, confint = TRUE, digits = 3, cluster=&quot;id&quot;, model.info = FALSE, model.fit = FALSE) ## Warning in !is.null(rmarkdown::metadata$output) &amp;&amp; rmarkdown::metadata$output ## %in% : &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; Est. 2.5% 97.5% z val. p (Intercept) -9.976 -10.602 -9.351 -31.251 0.000 t0 0.091 0.079 0.103 14.462 0.000 Z 0.183 -0.075 0.441 1.388 0.165 Standard errors: Cluster-robust, type = HC0 We can also do the same for OR: summ(ITTFit, robust = &quot;HC0&quot;, exp = TRUE, confint = TRUE, digits = 3, cluster=&quot;id&quot;, model.info = FALSE, model.fit = FALSE) exp(Est.) 2.5% 97.5% z val. p (Intercept) 0.000 0.000 0.000 -31.251 0.000 t0 1.095 1.082 1.109 14.462 0.000 Z 1.200 0.928 1.554 1.388 0.165 Standard errors: Cluster-robust, type = HC0 "],["naive-per-protocol-estimation.html", "Naive Per Protocol Estimation", " Naive Per Protocol Estimation The next effect estimate we calculate is the naive Per Protocol (naive PP) estimate. This corresponds to comparing all those who were randomized to receive the treatment, and truly did receive it against those who were randomized to receive the control and truly did receive the control. The estimation of this treatment effect corresponds to: \\[logit(Y) = \\gamma_{0} + \\gamma_{A} A + \\gamma_{t} t\\] For this estimate, we start by artificially censoring the dataset so that we only retain observations from individuals at time points where they were adherent to their randomized treatment. #### First artificially censor the original data within each arm simulated.data.treated &lt;- simulated.data[simulated.data$Z==1 &amp; simulated.data$Alag1==1,] simulated.data.control &lt;- simulated.data[simulated.data$Z==0 &amp; simulated.data$Alag1==0,] #### Join these to form the complete artificially censored dataset simulated.data.combined &lt;- rbind(simulated.data.treated, simulated.data.control) head(simulated.data.combined) ## Z id t0 B L1 L2 L2lag cavgL1 A Alag1 Y ## 1 1 1 0 0.2780523 2.64317182 1 0 2.643172 1 1 0 ## 2 1 1 1 0.2780523 6.14841344 1 1 4.395793 1 1 0 ## 3 1 1 2 0.2780523 1.56694834 1 1 3.452845 1 1 0 ## 4 1 1 3 0.2780523 -0.03690539 1 1 2.580407 1 1 0 ## 5 1 1 4 0.2780523 0.32103495 1 1 2.128533 1 1 0 ## 6 1 1 5 0.2780523 -2.22462997 0 1 1.403006 1 1 0 Next, we fit the pooled logistic regression model: naivePPFit &lt;- glm(Y ~ t0 + A, data = simulated.data.combined, family = binomial(link=&quot;logit&quot;)) naivePPFit ## ## Call: glm(formula = Y ~ t0 + A, family = binomial(link = &quot;logit&quot;), ## data = simulated.data.combined) ## ## Coefficients: ## (Intercept) t0 A ## -10.96716 0.09479 1.26137 ## ## Degrees of Freedom: 99371 Total (i.e. Null); 99369 Residual ## Null Deviance: 2365 ## Residual Deviance: 2051 AIC: 2057 Lastly, we calculate the appropriate standard error, p-values, and confidence intervals using the clustered sandwich standard errors: cov.m1 &lt;- vcovCL(naivePPFit, type = &quot;HC0&quot;, cluster = simulated.data.combined$id) co.test &lt;- coeftest(naivePPFit, vcov = cov.m1) co.CI &lt;- coefci(naivePPFit, vcov = cov.m1, level = 0.95) cat(&quot;Naive PP effect estimate:&quot;, &quot;\\n&quot;, &quot;log(OR)&quot;, round(coefficients(naivePPFit)[[&quot;A&quot;]],3), &quot;\\n&quot;, &quot;95% CI:&quot;, round(co.CI[&quot;A&quot;,&quot;2.5 %&quot;],3), &quot;,&quot;,round(co.CI[&quot;A&quot;,&quot;97.5 %&quot;],3), &quot;\\n&quot;, &quot;p-value&quot;, round(co.test[&quot;A&quot;,&quot;Pr(&gt;|z|)&quot;],3), &quot;\\n&quot;, &quot;robust standard error&quot;, round(sqrt(diag(cov.m1))[[&quot;A&quot;]],3)) ## Naive PP effect estimate: ## log(OR) 1.261 ## 95% CI: 0.895 , 1.628 ## p-value 0 ## robust standard error 0.187 The naive PP estimate concludes that the effect of treatment is significant, with a log odds ratio of 1.2613734. This result would indicate that the receipt of treatment is associated with an increase in the likelihood of the outcome occurring. However, naive PP estimates may be subject to bias if non-adherence is associated with the outcome of interest. Alternatively, we can again use the summ() function to obtain the same estimates: summ(naivePPFit, robust = &quot;HC0&quot;, confint = TRUE, digits = 3, cluster=&quot;id&quot;, model.info = FALSE, model.fit = FALSE) Est. 2.5% 97.5% z val. p (Intercept) -10.967 -11.746 -10.188 -27.605 0.000 t0 0.095 0.080 0.109 12.669 0.000 A 1.261 0.896 1.627 6.769 0.000 Standard errors: Cluster-robust, type = HC0 "],["adjusted-per-protocol-estimation.html", "Adjusted Per Protocol Estimation", " Adjusted Per Protocol Estimation The last estimates to calculate are the adjusted PP estimates, including the baseline adjusted and stabilized IPW adjusted PP estimates. These estimates aim to adjust for bias introduced by only analyzing the cohort of adherent patients. Baseline Adjusted Per Protocol Estimation Post-baseline Prognostic Factor Adjusted Per Protocol Estimation Inverse Probability Weighted Per Protocol Estimation "],["baseline-adjusted-per-protocol-estimation.html", "Baseline Adjusted Per Protocol Estimation", " Baseline Adjusted Per Protocol Estimation Baseline adjusted PP estimates use the subset of patient observations where treatments adhered to their randomly assigned treatments to calculate the effect estimate in the following form: \\[logit(Y) = \\gamma_{0} + \\gamma_{A} A + \\gamma_{B} B + \\gamma_{t} t\\] Using the artificially censored dataset of adherent patient observations, we fit the pooled logistic regression: BAdjPPFit &lt;- glm(Y ~ t0 + A + B, data = simulated.data.combined, family = binomial(link=&quot;logit&quot;)) BAdjPPFit ## ## Call: glm(formula = Y ~ t0 + A + B, family = binomial(link = &quot;logit&quot;), ## data = simulated.data.combined) ## ## Coefficients: ## (Intercept) t0 A B ## -15.7337 0.1027 0.6536 7.5560 ## ## Degrees of Freedom: 99371 Total (i.e. Null); 99368 Residual ## Null Deviance: 2365 ## Residual Deviance: 1802 AIC: 1810 Then similar to our previous models we calculate the clustered standard errors: cov.m1 &lt;- vcovCL(BAdjPPFit, type = &quot;HC0&quot;, cluster = simulated.data.combined$id) co.test &lt;- coeftest(BAdjPPFit, vcov = cov.m1) co.CI &lt;- coefci(BAdjPPFit, vcov = cov.m1, level = 0.95) cat(&quot;Baseline adjusted PP effect estimate:&quot;, &quot;\\n&quot;, &quot;log(OR)&quot;, round(coefficients(BAdjPPFit)[[&quot;A&quot;]],3), &quot;\\n&quot;, &quot;95% CI:&quot;, round(co.CI[&quot;A&quot;,&quot;2.5 %&quot;],3), &quot;,&quot;,round(co.CI[&quot;A&quot;,&quot;97.5 %&quot;],3), &quot;\\n&quot;, &quot;p-value&quot;, round(co.test[&quot;A&quot;,&quot;Pr(&gt;|z|)&quot;],3), &quot;\\n&quot;, &quot;robust standard error&quot;, round(sqrt(diag(cov.m1))[[&quot;A&quot;]],3)) ## Baseline adjusted PP effect estimate: ## log(OR) 0.654 ## 95% CI: 0.272 , 1.035 ## p-value 0.001 ## robust standard error 0.195 Alternatively, we can again use the summ() function to obtain the same estimates: summ(BAdjPPFit, robust = &quot;HC0&quot;, confint = TRUE, digits = 3, cluster=&quot;id&quot;, model.info = FALSE, model.fit = FALSE) Est. 2.5% 97.5% z val. p (Intercept) -15.734 -16.891 -14.576 -26.635 0.000 t0 0.103 0.087 0.118 13.110 0.000 A 0.654 0.280 1.027 3.433 0.001 B 7.556 6.464 8.648 13.558 0.000 Standard errors: Cluster-robust, type = HC0 "],["post-baseline-prognostic-factor-adjusted-per-protocol-estimation.html", "Post-baseline Prognostic Factor Adjusted Per Protocol Estimation", " Post-baseline Prognostic Factor Adjusted Per Protocol Estimation Post-baseline prognostic factor adjusted PP estimates use the subset of patient observations where treatments adhered to their randomly assigned treatments to calculate the effect estimate in the following form: \\[logit(Y) = \\gamma_{0} + \\gamma_{A} A + \\gamma_{L} L + \\gamma_{t} t\\] Using the artificially censored dataset of adherent patient observations, we fit the pooled logistic regression: LAdjPPFit &lt;- glm(Y ~ t0 + A + L1 + L2, data = simulated.data.combined, family = binomial(link=&quot;logit&quot;)) LAdjPPFit ## ## Call: glm(formula = Y ~ t0 + A + L1 + L2, family = binomial(link = &quot;logit&quot;), ## data = simulated.data.combined) ## ## Coefficients: ## (Intercept) t0 A L1 L2 ## -13.30911 0.09248 1.78801 0.22582 1.55136 ## ## Degrees of Freedom: 99371 Total (i.e. Null); 99367 Residual ## Null Deviance: 2365 ## Residual Deviance: 1921 AIC: 1931 Then similar to our previous models we calculate the clustered standard errors: cov.m1 &lt;- vcovCL(LAdjPPFit, type = &quot;HC0&quot;, cluster = simulated.data.combined$id) co.test &lt;- coeftest(LAdjPPFit, vcov = cov.m1) co.CI &lt;- coefci(LAdjPPFit, vcov = cov.m1, level = 0.95) cat(&quot;Post-baseline prognostic factor adjusted PP effect estimate:&quot;, &quot;\\n&quot;, &quot;log(OR)&quot;, round(coefficients(LAdjPPFit)[[&quot;A&quot;]],3), &quot;\\n&quot;, &quot;95% CI:&quot;, round(co.CI[&quot;A&quot;,&quot;2.5 %&quot;],3), &quot;,&quot;,round(co.CI[&quot;A&quot;,&quot;97.5 %&quot;],3), &quot;\\n&quot;, &quot;p-value&quot;, round(co.test[&quot;A&quot;,&quot;Pr(&gt;|z|)&quot;],3), &quot;\\n&quot;, &quot;robust standard error&quot;, round(sqrt(diag(cov.m1))[[&quot;A&quot;]],3)) ## Post-baseline prognostic factor adjusted PP effect estimate: ## log(OR) 1.788 ## 95% CI: 1.429 , 2.147 ## p-value 0 ## robust standard error 0.183 Alternatively, we can again use the summ() function to obtain the same estimates: summ(LAdjPPFit, robust = &quot;HC0&quot;, confint = TRUE, digits = 3, cluster=&quot;id&quot;, model.info = FALSE, model.fit = FALSE) Est. 2.5% 97.5% z val. p (Intercept) -13.309 -14.289 -12.329 -26.610 0.000 t0 0.092 0.078 0.107 12.238 0.000 A 1.788 1.405 2.171 9.141 0.000 L1 0.226 0.158 0.294 6.521 0.000 L2 1.551 0.961 2.142 5.152 0.000 Standard errors: Cluster-robust, type = HC0 "],["inverse-probability-weighted-per-protocol-estimation.html", "Inverse Probability Weighted Per Protocol Estimation Deriving Inverse Probability Weights Fitting Weighted Outcome Models", " Inverse Probability Weighted Per Protocol Estimation Deriving Inverse Probability Weights To fit the IPW-adjusted PP estimates, first, the weights are calculated for each observation. The stabilized weights for individual \\(i\\)’s \\(t\\)th observation is given by: \\[W_{i,t} = \\prod_{t=1}^{t} \\frac{P(A_{t} |B, t)}{P(A_{t} |B, t, L_{1,t}, L_{2,t})}\\] where: \\(P(A_{t} |B, t)\\) are the predicted values from logistic regression of the form: \\[logit(A_{t}) = \\gamma_{t} t + \\gamma_{B} B\\] and \\(P(A_{t} |B, t, L_{1,t}, L_{2, t})\\) are the predicted values from logistic regression of the form: \\[logit(A_{t}) = \\gamma_{t} t + \\gamma_{B} B + \\gamma_{L1} L_{1, t} + \\gamma_{L2} L_{2, t}\\] To calculate these weights, we censor the complete dataset to include adherent observations: #### First artificially censor the original data within each arm simulated.data.treated &lt;- simulated.data[simulated.data$Z==1 &amp; simulated.data$Alag1==1,] simulated.data.control &lt;- simulated.data[simulated.data$Z==0 &amp; simulated.data$Alag1==0,] We then fit the regression models for the numerator and the denominators, separately for each arm, starting with arm \\(Z=1\\): ##Create IP of artificial censoring weights ##For records with Z=1 ## Calculate numerator alpha_hat coefs for stabilized IPW numprob.tr &lt;- glm(A~t0+B, data=simulated.data.treated, family = binomial(link = &quot;logit&quot;)) ## Calculate denominator alpha_hat coefs for stabilized IPW denomprob.tr &lt;- glm(A~cavgL1+L2lag+t0+B, data=simulated.data.treated, family=binomial(link = &quot;logit&quot;)) Using the predicted probabilities, we can calculate the stabilized IPW, as well as the truncated stabilized IPW: ## Calculating weights from the predicted probabilities ## Derive the weight contributions at each time point simulated.data.treated &lt;- simulated.data.treated %&gt;% mutate(wgt_temp = if_else(A==1, predict(numprob.tr, simulated.data.treated, type = &quot;response&quot;) / predict(denomprob.tr, simulated.data.treated, type = &quot;response&quot;), 0)) %&gt;% ## Group data by patient group_by(id) %&gt;% ## Cumulative multiply the weight contributions within each patient mutate(sipw = cumprod(wgt_temp)) %&gt;% ungroup() %&gt;% ## Create truncated IPW weights mutate(tsipw = pmin(pmax(sipw, quantile(sipw, .05)), quantile(sipw, .95))) Then we repeat the process for arm \\(Z=0\\) by fitting the regression models: ##For records with Z=0 ## Calculate numerator alpha_hat coefs for stabilized IPW numprob.cntr &lt;- glm(A~t0+B, data=simulated.data.control, family = binomial(link = &quot;logit&quot;)) ## Calculate denominator alpha_hat coefs for stabilized IPW denomprob.cntr &lt;- glm(A~cavgL1+L2lag+t0+B, data=simulated.data.control, family=binomial(link = &quot;logit&quot;)) And using the predicted probabilities to generate the stabilized and truncated stabilized IPW: ## Calculating weights from the predicted probabilities ## Derive the weight contributions at each time point simulated.data.control &lt;- simulated.data.control %&gt;% mutate(wgt_temp = if_else(A==0, 1- predict(numprob.cntr, simulated.data.control, type = &quot;response&quot;) / 1- predict(denomprob.cntr, simulated.data.control, type = &quot;response&quot;), 0)) %&gt;% ## Group data by patient group_by(id) %&gt;% ## Cumulative multiply the weight contributions within each patient mutate(sipw = cumprod(wgt_temp)) %&gt;% ungroup() %&gt;% ## Create truncated IPW weights mutate(tsipw = pmin(pmax(sipw, quantile(sipw, .05)), quantile(sipw, .95))) Now we can assess the weights by checking the mean and standard deviation of the stabilized vs. the truncated stabilized weights: ## Assess the weights generated ## Join arm 1 and arm 2 datasets simulated.data.combined &lt;- rbind(simulated.data.control, simulated.data.treated) simulated.data.combined %&gt;% summarize(mean_SIPW = mean(sipw), sd_SIPW = sd(sipw), mean_TSIPW = mean(tsipw), sd_TWIPW = sd(tsipw)) ## # A tibble: 1 × 4 ## mean_SIPW sd_SIPW mean_TSIPW sd_TWIPW ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.895 0.198 0.899 0.172 We see the weights are reasonably close to 1, and that truncation reduces the standard deviation of the weights. Fitting Weighted Outcome Models To get the effect estimates, we perform weighted pooled logistic regression according to the following model: \\[ logit(Y) = \\gamma_{0} + \\gamma_{A} A + \\gamma_{t} t + \\gamma_{B} B\\] SIPWFit &lt;- glm(Y ~ t0 + A + B, data = simulated.data.combined, family = binomial(link=&quot;logit&quot;), weight = sipw) ## use start = rep(.001,4), control = list(maxit = 250) for identity link/reporting RD SIPWFit ## ## Call: glm(formula = Y ~ t0 + A + B, family = binomial(link = &quot;logit&quot;), ## data = simulated.data.combined, weights = sipw) ## ## Coefficients: ## (Intercept) t0 A B ## -15.9033 0.1038 0.6881 7.6721 ## ## Degrees of Freedom: 98793 Total (i.e. Null); 98790 Residual ## Null Deviance: 2014 ## Residual Deviance: 1496 AIC: 1478 cov.m1 &lt;- vcovCL(SIPWFit, type = &quot;HC0&quot;, cluster = simulated.data.combined$id) co.test &lt;- coeftest(SIPWFit, vcov = cov.m1) co.CI &lt;- coefci(SIPWFit, vcov = cov.m1, level = 0.95) cat(&quot;Stabilized IPW adjusted PP effect estimate:&quot;, &quot;\\n&quot;, &quot;log(OR)&quot;, round(coefficients(SIPWFit)[[&quot;A&quot;]],3), &quot;\\n&quot;, &quot;95% CI:&quot;, round(co.CI[&quot;A&quot;,&quot;2.5 %&quot;],3), &quot;,&quot;,round(co.CI[&quot;A&quot;,&quot;97.5 %&quot;],3), &quot;\\n&quot;, &quot;p-value&quot;, round(co.test[&quot;A&quot;,&quot;Pr(&gt;|z|)&quot;],3), &quot;\\n&quot;, &quot;robust standard error&quot;, round(sqrt(diag(cov.m1))[[&quot;A&quot;]],3)) ## Stabilized IPW adjusted PP effect estimate: ## log(OR) 0.688 ## 95% CI: 0.244 , 1.133 ## p-value 0.002 ## robust standard error 0.227 Alternatively, we can again use the summ() function to obtain the same estimates: summ(SIPWFit, robust = &quot;HC0&quot;, confint = TRUE, digits = 3, cluster=&quot;id&quot;, model.info = FALSE, model.fit = FALSE) Est. 2.5% 97.5% z val. p (Intercept) -15.903 -17.151 -14.656 -24.990 0.000 t0 0.104 0.087 0.120 12.150 0.000 A 0.688 0.100 1.276 2.294 0.022 B 7.672 6.467 8.877 12.478 0.000 Standard errors: Cluster-robust, type = HC0 Note that, this model can alternatively use the truncated stabilized weights by switching the weight used in the regression calculation from sipw to tsipw. "],["compare-results.html", "Compare Results", " Compare Results Comparing log(OR) from all models (vertical line represents the true parameter for log(OR) when we generated the simulated data): plot_summs(ITTFit, naivePPFit, BAdjPPFit, SIPWFit, scale = TRUE, robust = list(&quot;HC0&quot;,&quot;HC0&quot;,&quot;HC0&quot;, &quot;HC0&quot;, &quot;HC0&quot;), model.names = c(&quot;ITT&quot;, &quot;PP&quot;, &quot;B-PP&quot;, &quot;IPW-PP&quot;)) + geom_vline(xintercept = 0.7) Comparing OR from all models (vertical line represents the true parameter for OR when we generated the simulated data): plot_summs(ITTFit, naivePPFit, BAdjPPFit, SIPWFit, scale = TRUE, exp = TRUE, robust = list(&quot;HC0&quot;,&quot;HC0&quot;,&quot;HC0&quot;, &quot;HC0&quot;, &quot;HC0&quot;), model.names = c(&quot;ITT&quot;, &quot;PP&quot;, &quot;B-PP&quot;, &quot;IPW-PP&quot;)) + geom_vline(xintercept = exp(0.7)) In all of these cases, estimated parameters associated with A are of main interest. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
