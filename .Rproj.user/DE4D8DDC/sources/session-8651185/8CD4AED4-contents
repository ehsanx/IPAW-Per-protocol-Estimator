--- 
title: "Illustrative example of Per-protocol effect estimation in R"
author: "Lucy Mosquera & Ehsan Karim"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::html_document2:
    includes:
      in_header: header.html
      css: [style.css]
  bookdown::gitbook:
    includes:
      in_header: header.html
  bookdown::pdf_book:
    includes:
      in_header: header.html
documentclass: book
link-citations: yes
github-repo: ehsanx/IPAW-Per-protocol-Estimator
description: "Implementing TMLE"
header-includes: 
  - \usepackage{tcolorbox}
  - \newtcolorbox{blackbox}{colback=black,colframe=orange,coltext=white,boxsep=5pt,arc=4pt}
  - \usepackage{color}
  - \usepackage{framed}
  - \setlength{\fboxsep}{.8em}
---

\newenvironment{blackbox}{
  \definecolor{shadecolor}{rgb}{0, 0, 0}  % black
  \color{white}
  \begin{shaded}}
 {\end{shaded}}

# Citation {-}

Supplementary materials. Check the main article for the analytic and theoretical details, and complete citations from the literature.

```{block, type='rmdcomment'}
**How to cite**

Mosquera, L and Karim, ME (2022) "Illustrative example of Per-protocol effect estimation in R", URL: [ehsanx.github.io/IPAW-Per-protocol-Estimator](https://ehsanx.github.io/IPAW-Per-protocol-Estimator)
```

<!--chapter:end:index.Rmd-->

# Purpose {-}

The purpose of this supplement is to illustrate the estimators used in the article. This code and associated example dataset, is based on the Diagram 1 (i) data generating mechanism/simulation outlined in our manuscript; with true `log(OR)` = 0.7, and show how to calculate intention to treat (ITT), naive per-protocol (naive PP), baseline adjusted per-protocol and stabilized inverse probability weighted (IPW) adjusted per-protocol effect estimates. These estimators can be used for analyzing pragmatic trials in the presence of non-adherence. All computations were done in R version 3.6.1. 

<!--chapter:end:01Purpose.Rmd-->

# Data Setup  {-}

The analyses conducted in this supplement will require loading the following packages:

```{r, warning=FALSE}
## Load required packages
suppressPackageStartupMessages(library(lmtest))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(sandwich))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggstance))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(jtools))
```

Next, by setting the `filePath` variable to match the [location](https://github.com/ehsanx/IPAW-Per-protocol-Estimator) of the supplied `Pragmatic Trial Data.csv`, we can open the dataset. By printing the first few rows, we can start getting ourselves familiar with the data.

```{r, eval=TRUE, include=FALSE, cache=TRUE}
## Load the dataset
simulated.data <- read.csv("data/Pragmatic Trial Data.csv")
```

```{r, eval = FALSE}
## Load the dataset
simulated.data <- read.csv(paste0(filePath, "data/Pragmatic Trial Data.csv"))
```

```{r, cache=TRUE}
head(simulated.data)
```


We see that this dataset consists of 11 variables, including:

- Z the arm the patient was randomized to
- id the patient id that groups observations from the same individual together
- t0 the time point of the observation
- B the measured baseline covariate
- L1 and L2 the time-varying covariates, with L2lag and cavgL1 derived from these columns
- A whether the patient received the treatment for the time interval prior to this observation
- Y whether the individual experienced the outcome of interest

<!--chapter:end:02DataSetup.Rmd-->

# Data Exploration {-}

To better understand the dataset, we can explore the distribution of patients and observations per patient.

```{r, cache=TRUE}
## Trial characteristics
n <- length(unique(simulated.data$id))
n1 <- length(unique(simulated.data$id[simulated.data$Z==1]))
n0 <- length(unique(simulated.data$id[simulated.data$Z==0]))
averageObs <- mean(table(simulated.data$id))

cat(n, "participants total", "\n", 
		n1, "were randomized to the treatment arm", "\n",
		n0, "were randomized to the control arm.", "\n",
		"On average each patient has ", round(averageObs, 1), "observations")
```

There are 1000 individuals in each treatment arm with approximately 59 observations per person. The proportion of trial participants who experience the event of interest in the trial dataset is:

```{r, cache=TRUE}
## Event rate
e.rate <- sum(simulated.data$Y)/
  length(unique(simulated.data$id))
cat("Event rate is", round(e.rate*100,1), "%\n")
```

Next, we can quantify non-adherence within the trial by calculating what proportion of individuals became non-adherent by their last observation:

```{r, cache=TRUE}
## Non-adherence rates by arm

## how many person-time deviated in the treatment arm?
simulated.data.treated <- simulated.data[simulated.data$Z==1 & 
                                           simulated.data$Alag1==1,]
## how many person-time deviated in the control arm?
simulated.data.control <- simulated.data[simulated.data$Z==0 & 
                                           simulated.data$Alag1==0,]

pt1dev = dim(simulated.data.treated[simulated.data.treated$A==0&
                                      simulated.data.treated$t0<59,])[1]/n1
pt0dev = dim(simulated.data.control[simulated.data.control$A==1&
                                      simulated.data.control$t0<59,])[1]/n0

cat("In the treatment arm", pt1dev*100, "% were non-adherent by their last observation", 
		"\n", "In the control arm", pt0dev*100, 
		"% were non-adherent by their last observation")
```

As the last part of our exploration, let's see the covariates for this dataset and how they are distributed between the two treatment arms. In this example trial, there is one measured baseline covariate and two time-varying covariates. The estimation methods detailed in this document can be expanded to handle additional baseline or time-varying covariates by using the complete set of observed baseline covariates wherever $B$ is used, and the complete set of observed time-varying covariates whereever $L_{1}$ and $L_{2}$ are used.

```{r, cache=TRUE}
simulated.data %>% 
	group_by(Z) %>% 
	summarize(B_mean = mean(B),
						B_sd = sd(B),
						L1_mean = mean(L1),
						L1_sd = sd(L1),
						L2_prop = sum(L2)/n())
	
```

We can see the baseline covariate is well balanced between the two treatment arms while the time-varying covariates are not well balanced between the treatment arms. This can occur when the time-varying covariates are associated with the receipt of treatment.

<!--chapter:end:03DataExploration.Rmd-->

# Intention to Treat Estimation {-}

The first treatment effect is the Intention To Treat (ITT) effect estimate. This quantifies the causal effect of being randomized to the treatment arm vs. the control arm. In trials with perfect adherence, where all patients receive the treatment they are randomized to receive, this corresponds with the causal effect of receiving treatment. 

To estimate the ITT effect, we use pooled logistic regression of the form:

$$logit(Y) = \gamma_{0} + \gamma_{Z} Z + \gamma_{t}t.$$

Which is estimated via the following code:

```{r, cache=TRUE}
ITTFit <- glm(Y ~ t0 + Z, 
              data = simulated.data, 
              family = binomial(link="logit"))
ITTFit
```

To calculate the appropriate standard error, p-values, and confidence intervals, extra consideration must be used to account for the clustering of observations in the dataset. As we are using pooled logistic regression, each observation corresponds to a single individual at a given follow-up. This means that observations are not all independent, as we'd expect observations from the same individual to be more alike other than observations from different individuals. We calculate the appropriate uncertainty for each estimate using clustered sandwich estimates of the standard error:

```{r, cache=TRUE}
cov.m1 <- vcovCL(ITTFit, type = "HC0", cluster= simulated.data$id)
co.test <- coeftest(ITTFit, vcov = cov.m1)
co.CI <- coefci(ITTFit, vcov = cov.m1, level = 0.95)
cat("ITT effect estimate:", "\n",
		"log(OR)", round(coefficients(ITTFit)[["Z"]],3), "\n",
	  "95% CI:", round(co.CI["Z","2.5 %"],3), ",", round(co.CI["Z","97.5 %"],3), "\n",
	  "p-value", round(co.test["Z","Pr(>|z|)"],2), "\n",
	  "robust standard error", round(sqrt(diag(cov.m1))[["Z"]],3))
```

Our ITT estimate concludes that treatment does not have a statistically significant effect on the outcome. We have showed the calculation above in details to explain how exactly clustered sandwich estimates of the standard error are calculated to make proper inference. 

Alternatively, we can use `summ()` function from `jtools` package to perform the above calculations for `log(OR)` in a straightforward manner by specifying appropriate arguments.

```{r, cache=TRUE}
require(jtools)
summ(ITTFit, robust = "HC0", 
     confint = TRUE, digits = 3, cluster="id", 
     model.info = FALSE, model.fit = FALSE)
```

We can also do the same for `OR`:

```{r, cache=TRUE}
summ(ITTFit, robust = "HC0", exp = TRUE,
     confint = TRUE, digits = 3, cluster="id", 
     model.info = FALSE, model.fit = FALSE)
```

<!--chapter:end:04ITT.Rmd-->

# Naive Per Protocol Estimation {-}

The next effect estimate we calculate is the naive Per Protocol (naive PP) estimate. This corresponds to comparing all those who were randomized to receive the treatment, and truly did receive it against those who were randomized to receive the control and truly did receive the control. 

The estimation of this treatment effect corresponds to:

$$logit(Y) = \gamma_{0} + \gamma_{A} A + \gamma_{t} t$$ 

For this estimate, we start by artificially censoring the dataset so that we only retain observations from individuals at time points where they were adherent to their randomized treatment. 

```{r, cache=TRUE}
#### First artificially censor the original data within each arm
simulated.data.treated <- simulated.data[simulated.data$Z==1 & 
                                           simulated.data$Alag1==1,]
simulated.data.control <- simulated.data[simulated.data$Z==0 & 
                                           simulated.data$Alag1==0,]

#### Join these to form the complete artificially censored dataset
simulated.data.combined <- rbind(simulated.data.treated, 
                                 simulated.data.control)
head(simulated.data.combined)
```

Next, we fit the pooled logistic regression model:

```{r, cache=TRUE}
naivePPFit <- glm(Y ~ t0 + A, 
                  data = simulated.data.combined, 
                  family = binomial(link="logit"))
naivePPFit
```

Lastly, we calculate the appropriate standard error, p-values, and confidence intervals using the clustered sandwich standard errors:

```{r, cache=TRUE}
cov.m1 <- vcovCL(naivePPFit, type = "HC0", 
                 cluster = simulated.data.combined$id)
co.test <- coeftest(naivePPFit, vcov = cov.m1)
co.CI <- coefci(naivePPFit, vcov = cov.m1, level = 0.95)
cat("Naive PP effect estimate:", "\n",
		"log(OR)", round(coefficients(naivePPFit)[["A"]],3), "\n",
	  "95% CI:", round(co.CI["A","2.5 %"],3), ",",round(co.CI["A","97.5 %"],3), "\n",
	  "p-value", round(co.test["A","Pr(>|z|)"],3), "\n",
	  "robust standard error", round(sqrt(diag(cov.m1))[["A"]],3))
```

The naive PP estimate concludes that the effect of treatment is significant, with a log odds ratio of `r coefficients(naivePPFit)[["A"]]`. This result would indicate that the receipt of treatment is associated with an increase in the likelihood of the outcome occurring. However, naive PP estimates may be subject to bias if non-adherence is associated with the outcome of interest. 

Alternatively, we can again use the `summ()` function to obtain the same estimates:

```{r, cache=TRUE}
summ(naivePPFit, robust = "HC0", 
     confint = TRUE, digits = 3, cluster="id", 
     model.info = FALSE, model.fit = FALSE)
```

<!--chapter:end:05nPP.Rmd-->

# Adjusted Per Protocol Estimation {-}

The last estimates to calculate are the adjusted PP estimates, including the baseline adjusted and stabilized IPW adjusted PP estimates. These estimates aim to adjust for bias introduced by only analyzing the cohort of adherent patients.

- Baseline Adjusted Per Protocol Estimation
- Post-baseline Prognostic Factor Adjusted Per Protocol Estimation
- Inverse Probability Weighted Per Protocol Estimation

<!--chapter:end:06aPP.Rmd-->

# Baseline Adjusted Per Protocol Estimation {-}

Baseline adjusted PP estimates use the subset of patient observations where treatments adhered to their randomly assigned treatments to calculate the effect estimate in the following form:

$$logit(Y) = \gamma_{0} + \gamma_{A} A + \gamma_{B} B + \gamma_{t} t$$

Using the artificially censored dataset of adherent patient observations, we fit the pooled logistic regression:

```{r, cache=TRUE}
BAdjPPFit <- glm(Y ~ t0 + A + B, data = 
                   simulated.data.combined, 
                 family = binomial(link="logit"))
BAdjPPFit
```

Then similar to our previous models we calculate the clustered standard errors:

```{r, cache=TRUE}
cov.m1 <- vcovCL(BAdjPPFit, type = "HC0", 
                 cluster = simulated.data.combined$id) 
co.test <- coeftest(BAdjPPFit, vcov = cov.m1)
co.CI <- coefci(BAdjPPFit, vcov = cov.m1, level = 0.95)

cat("Baseline adjusted PP effect estimate:", "\n",
		"log(OR)", round(coefficients(BAdjPPFit)[["A"]],3), "\n",
	  "95% CI:", round(co.CI["A","2.5 %"],3), ",",round(co.CI["A","97.5 %"],3), "\n",
	  "p-value", round(co.test["A","Pr(>|z|)"],3), "\n",
	  "robust standard error", round(sqrt(diag(cov.m1))[["A"]],3))

```

Alternatively, we can again use the `summ()` function to obtain the same estimates:

```{r, cache=TRUE}
summ(BAdjPPFit, robust = "HC0", 
     confint = TRUE, digits = 3, cluster="id", 
     model.info = FALSE, model.fit = FALSE)
```

<!--chapter:end:07BaPP.Rmd-->

# Post-baseline Prognostic Factor Adjusted Per Protocol Estimation {-}

Post-baseline prognostic factor adjusted PP estimates use the subset of patient observations where treatments adhered to their randomly assigned treatments to calculate the effect estimate in the following form:

$$logit(Y) = \gamma_{0} + \gamma_{A} A + \gamma_{L} L + \gamma_{t} t$$

Using the artificially censored dataset of adherent patient observations, we fit the pooled logistic regression:

```{r, cache=TRUE}
LAdjPPFit <- glm(Y ~ t0 + A + L1 + L2, data = 
                   simulated.data.combined, 
                 family = binomial(link="logit"))
LAdjPPFit
```

Then similar to our previous models we calculate the clustered standard errors:

```{r, cache=TRUE}
cov.m1 <- vcovCL(LAdjPPFit, type = "HC0", 
                 cluster = simulated.data.combined$id) 
co.test <- coeftest(LAdjPPFit, vcov = cov.m1)
co.CI <- coefci(LAdjPPFit, vcov = cov.m1, level = 0.95)

cat("Post-baseline prognostic factor adjusted PP effect estimate:", "\n",
		"log(OR)", round(coefficients(LAdjPPFit)[["A"]],3), "\n",
	  "95% CI:", round(co.CI["A","2.5 %"],3), ",",round(co.CI["A","97.5 %"],3), "\n",
	  "p-value", round(co.test["A","Pr(>|z|)"],3), "\n",
	  "robust standard error", round(sqrt(diag(cov.m1))[["A"]],3))

```

Alternatively, we can again use the `summ()` function to obtain the same estimates:

```{r, cache=TRUE}
summ(LAdjPPFit, robust = "HC0", 
     confint = TRUE, digits = 3, cluster="id", 
     model.info = FALSE, model.fit = FALSE)
```

<!--chapter:end:08LaPP.Rmd-->

# Inverse Probability Weighted Per Protocol Estimation {-}

## Deriving Inverse Probability Weights {-}

To fit the IPW-adjusted PP estimates, first, the weights are calculated for each observation. The stabilized weights for individual $i$'s $t$th  observation is given by:


$$W_{i,t} = \prod_{t=1}^{t} \frac{P(A_{t} |B, t)}{P(A_{t} |B, t, L_{1,t}, L_{2,t})}$$
where: $P(A_{t} |B, t)$ are the predicted values from logistic regression of the form:

$$logit(A_{t}) = \gamma_{t} t + \gamma_{B} B$$

and $P(A_{t} |B, t, L_{1,t}, L_{2, t})$ are the predicted values from logistic regression of the form:

$$logit(A_{t}) = \gamma_{t} t + \gamma_{B} B + \gamma_{L1} L_{1, t} + \gamma_{L2} L_{2, t}$$

To calculate these weights, we censor the complete dataset to include adherent observations:

```{r, cache=TRUE}
#### First artificially censor the original data within each arm
simulated.data.treated <- simulated.data[simulated.data$Z==1 & 
                                           simulated.data$Alag1==1,]
simulated.data.control <- simulated.data[simulated.data$Z==0 & 
                                           simulated.data$Alag1==0,]
```

We then fit the regression models for the numerator and the denominators, separately for each arm, starting with arm $Z=1$:

```{r, cache=TRUE}
##Create IP of artificial censoring weights
##For records with Z=1 
  
## Calculate numerator alpha_hat coefs for stabilized IPW
numprob.tr <- glm(A~t0+B, 
                 data=simulated.data.treated, 
                 family = binomial(link = "logit"))
  
## Calculate denominator alpha_hat coefs for stabilized IPW
denomprob.tr <- glm(A~cavgL1+L2lag+t0+B, 
                   data=simulated.data.treated, 
                   family=binomial(link = "logit"))
```

Using the predicted probabilities, we can calculate the stabilized IPW, as well as the truncated stabilized IPW:

```{r, cache=TRUE}
## Calculating weights from the predicted probabilities
## Derive the weight contributions at each time point
simulated.data.treated <- simulated.data.treated %>%
	mutate(wgt_temp = if_else(A==1, 
	                          predict(numprob.tr, 
	                                  simulated.data.treated, 
	                                  type = "response") / 
														  predict(denomprob.tr, 
														          simulated.data.treated, 
														          type = "response"),
														0)) %>% 
	## Group data by patient
	group_by(id) %>% 
	## Cumulative multiply the weight contributions within each patient
	mutate(sipw = cumprod(wgt_temp)) %>% 
	ungroup() %>% 
	## Create truncated IPW weights
	mutate(tsipw = pmin(pmax(sipw, quantile(sipw, .05)), 
											quantile(sipw, .95)))

```

Then we repeat the process for arm $Z=0$ by fitting the regression models:

```{r, cache=TRUE}
##For records with Z=0 
  
## Calculate numerator alpha_hat coefs for stabilized IPW
numprob.cntr <- glm(A~t0+B, 
                 data=simulated.data.control, 
                 family = binomial(link = "logit"))
## Calculate denominator alpha_hat coefs for stabilized IPW
denomprob.cntr <- glm(A~cavgL1+L2lag+t0+B, 
                   data=simulated.data.control, 
                   family=binomial(link = "logit"))
```

And using the predicted probabilities to generate the stabilized and truncated stabilized IPW:

```{r, cache=TRUE}
## Calculating weights from the predicted probabilities
## Derive the weight contributions at each time point 
simulated.data.control <- simulated.data.control %>% 
	mutate(wgt_temp = if_else(A==0, 1- predict(numprob.cntr, 
	                                           simulated.data.control, 
	                                           type = "response") / 
														  1- predict(denomprob.cntr, 
														             simulated.data.control, 
														             type = "response"),
														0)) %>% 
	## Group data by patient
	group_by(id) %>%
	## Cumulative multiply the weight contributions within each patient
	mutate(sipw = cumprod(wgt_temp)) %>% 
	ungroup() %>% 
	## Create truncated IPW weights
	mutate(tsipw = pmin(pmax(sipw, quantile(sipw, .05)), 
											quantile(sipw, .95)))  

```

Now we can assess the weights by checking the mean and standard deviation of the stabilized vs. the truncated stabilized weights:

```{r, cache=TRUE}
## Assess the weights generated

## Join arm 1 and arm 2 datasets
simulated.data.combined <- rbind(simulated.data.control, 
                                 simulated.data.treated)

simulated.data.combined %>%
	summarize(mean_SIPW = mean(sipw),
						sd_SIPW = sd(sipw),
						mean_TSIPW = mean(tsipw),
						sd_TWIPW = sd(tsipw))
```

We see the weights are reasonably close to 1, and that truncation reduces the standard deviation of the weights. 

## Fitting Weighted Outcome Models {-}

To get the effect estimates, we perform weighted pooled logistic regression according to the following model:

$$ logit(Y) = \gamma_{0} + \gamma_{A} A + \gamma_{t} t + \gamma_{B} B$$



```{r, warning = FALSE, cache=TRUE}
SIPWFit <- glm(Y ~ t0 + A + B, 
               data = simulated.data.combined, 
               family = binomial(link="logit"), 
							 weight = sipw)
## use start = rep(.001,4), control = list(maxit = 250) for identity link/reporting RD
SIPWFit
```

```{r, cache=TRUE}
cov.m1 <- vcovCL(SIPWFit, type = "HC0", cluster = simulated.data.combined$id)
co.test <- coeftest(SIPWFit, vcov = cov.m1)
co.CI <- coefci(SIPWFit, vcov = cov.m1, level = 0.95)
      
cat("Stabilized IPW adjusted PP effect estimate:", "\n",
		"log(OR)", round(coefficients(SIPWFit)[["A"]],3), "\n",
	  "95% CI:", round(co.CI["A","2.5 %"],3), ",",round(co.CI["A","97.5 %"],3), "\n",
	  "p-value", round(co.test["A","Pr(>|z|)"],3), "\n",
	  "robust standard error", round(sqrt(diag(cov.m1))[["A"]],3))
  
```

Alternatively, we can again use the `summ()` function to obtain the same estimates:

```{r, cache=TRUE}
summ(SIPWFit, robust = "HC0", 
     confint = TRUE, digits = 3, cluster="id", 
     model.info = FALSE, model.fit = FALSE)
```

Note that, this model can alternatively use the truncated stabilized weights by switching the weight used in the regression calculation from `sipw` to `tsipw`. 



<!--chapter:end:09IPWaPP.Rmd-->

# Compare Results {-}

Comparing `log(OR)` from all models (vertical line represents the true parameter for `log(OR)` when we generated the simulated data):

```{r, cache=TRUE, message=FALSE, warning=FALSE}
plot_summs(ITTFit, naivePPFit, BAdjPPFit, SIPWFit,
           scale = TRUE,
           coefs = "A",
           robust = list("HC0","HC0","HC0", "HC0", "HC0"),
           model.names = c("ITT", "PP", "B-PP", "IPW-PP")) +
geom_vline(xintercept = 0.7)
```

Comparing `OR` from all models (vertical line represents the true parameter for `OR` when we generated the simulated data):

```{r, cache=TRUE, message=FALSE, warning=FALSE}
plot_summs(ITTFit, naivePPFit, BAdjPPFit, SIPWFit,
           scale = TRUE,
           coefs = "A",
           exp = TRUE,
           robust = list("HC0","HC0","HC0", "HC0", "HC0"),
           model.names = c("ITT", "PP", "B-PP", "IPW-PP")) +
geom_vline(xintercept = exp(0.7))
```

<!--chapter:end:10compare.Rmd-->

